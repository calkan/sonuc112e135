\section{Giriş}

DNA dizileme, 1977'de Sanger metodu \cite{Sanger1977} ile ilk kez bir genomun tam diziliminin yapılmasının ardından uzun bir yol kat etti. Bu gelişmeden sonra genetik biliminin odak noktası tüm bir DNA'yı dizilemek oldu \cite{Kircher2010}. Human Genome Project (HGP) adı verilen proje, bu ilginin sonucu olarak ortaya çıkmış bir projedir. HGP'in tamamlanması yaklaşık olarak 13 yıl sürdü ve yaklaşık olarak 3 milyar dolara mal oldu \cite{genomep}. Projenin sonunda, insan genomundaki bütün genlerin belirlenmesi ve haritalandırılması başarıldı. HGP'den sonra ise yeni dizileme teknolojileri ortaya çıkmaya başladı. Gelecek kuşak dizileme teknolojilerinin ortaya çıkması ile birlikte \cite{Bentley2008, Clarke2009, Harris2008, Margulies2005, Shendure2005, Korlach2008},  günler içinde ve 1000 dolar civarında bir bütçe ile, tüm insan genomunu dizilemek mümkün olmaya başladı\cite{markoff}. Bu gelecek kuşak dizileme teknolojilerinin ortak zayıf noktası ise bütün bir dizileme çıktısından ziyade 50 ile 15000 baz çifti uzunluğunda parçalar halinde çıktılar oluşturması \cite{Mardis2008a}. Bundan dolayı bu parçaların uzunluğu kısaldıkça, tekrarlayan parçalar bulma şansı artacaktır. Genom birleştirme, gelecek kuşak teknolojileri tarafından üretilen parçaların tekrar bir araya getirilip bütün bir genomu birleştirme sürecidir. Sanger dizileme metodunu düşünecek olursak, bütün bir genomu küçük parçalardan birleştirmeye çalışmak daha kırılgan yapıda bir birleşmeye yol açacaktır \cite{Schatz2010}.

Kısa parçaların oluşturabileceği muhtemel zorluklar, dizileme teknolojilerinin daha uzun boyutlu parçalar üretmeye odaklanmasına sebep oldu. Bu teknolojilerden biri ise Pacific Biosciences'ın (PacBio) gerçek zamanlı dizileme teknolojisidir \cite{Eid2009}. PacBio 50000 baz çifti uzunluğuna kadar parçalar üretmeyi başarabiliyor \cite{Lee2014}. PacBio'nun bu teknolojisi her ne kadar muhtemelen tekrarlamaları, daha uzun parçalar üreterek en aza indirmeye çalışsa da uzun parçalar daha yüksek hata paylarına sahiptirler \cite{Ono2013}. Bundan dolayı uzun parçalar üreten teknolojiler hata oranını düşürmek için ilave algoritmalara ihtiyaç duyuyorlar.

Gelecek kuşak dizileme teknolojileri arasında market lideri ise Illumina/Solexa Genome Analyser (GA) olarak kabul edilmektedir \cite{Metzker2010}. Illumina/Solexa GA, PacBio'nun ürettiği dizilim parçalarına kıyasla çok daha yüksek kalitede ve birkaç yüz baz çifti uzunluğunda parçalar üretebilmektedir \cite{Metzker2010}. Bundan dolayı Illumina GA, variant keşfi için daha kullanışlıdır. Ancak, Illumina/Solexa sadece birkaç yüz baz çifti uzunluğunda parçalar ürettiğinden \cite{Shendure2008}, Illumina ile tekrarlayan parçalara sahip olma şansı PacBio'nun ürettiği dizilim parçalarına kıyasla daha yüksektir.

PacBio'nun ilerde sunacağı iyileştirmelerin daha yüksek kalitede parçalar üretmeye başlayacağını varsaysak dahi genom birleştirme süreci üretilen parçaların teker teker bütün karşılaştırma kombinasyonlarını deneyeceğinden dolayı işlemcinin \%95 toplam çalışma süresini kapsamaktadır \cite{Berlin2015}. Bizim çalışmamızda, genom birleşimi yapmak için gereken kısa ve uzun dizilim parçalarının karşılaştırma sayısını kısaltmayı amaçlayan bir çözüm öneriyoruz. Bunun için yöntemimiz ise seçilen filtrelerle gereksiz olması muhtemel karşılaştırmaları elemektir. Bunun yanında, uzun parçalardaki hata payını düşürmek için ise Illumina tarafından üretilen kısa parçalar ile PacBio tarafından üretilen uzun parçaları birbirleriyle karşılaştırıyoruz.

\section{Problem Tanımı}

Gelecek kuşak dizileme teknolojileri bütün bir genom dizilimini oluşturamadığından dolayı, bütün bir genomu yaratabilmek için üretilen parçalar arasındaki çakışmaların bulunması gerekiyor. Bu çakışma noktaları, iki parça arasında bir skor belirlemek için tanımlanıyor \cite{Staden1980}. İki parça arasında en yüksek skor bulunduğu zaman, iki parçayı düzgün bir pozisyonda hizalamak mümkün olabilir \cite{Staden1980}. Smith-Waterman (SW) dinamik programlama metodu, parçalar arasında hizalandırma yapmak için bilinen en hızlı metottur \cite{Smith1981}. $L$ uzunluğunda iki parçayı hizalandırmak için, SW metodunun hesapsal karmaşıklığı O($L^2$) olarak belirlenmiştir \cite{Smith1981}. Bundan dolayı, eğer toplamda $N$ sayıda ve herbiri $L$ uzunluğunda parçalara sahipsek, bütün hizalandırmaları bulmanın hesapsal karmaşıklığı O($N^2$$L^2$) olacaktır.

Belirtilen bu hesapsal karmaşıklık şu anki gelecek kuşak dizileme teknolojilerini üzerinde uygulanabilir düzeyde değildir. Yukarıda belirtilen hesapsal karmaşıklıktan dolayı Illumina ile üretilen parçaları hizalandırmak çok yüksek miktarda bir hesaplama gerektirecektir bunun nedeni ise Illumina'nın ortalama olarak 1 ile 3 milyar arasında parça üretiyor olmasıdır \cite{Metzker2010}. Bu hesapsal karmaşıklığın oluşmasında dikkat edilmesi gereken iki husus vardır. Birincisi, bir parçayı diğer bütün $N$ adet parça ile karşılaştırdığımızdan dolayı ortaya çıkan $N^2$ adet karmaşıklık. Bir diğeri ise $L$ uzunluğunda bir parçanın diğer $L$ uzunluğunda bir parçayla her bir karakterinin teker teker karşılaştırmasından ortaya çıkan $L^2$ karmaşıklıktır. Bundan dolayı, hesapsal karmaşıklığı düşürebilmek için hem karakter ölçeğinde hem de parçalar arasındaki karşılaştırmalardan teker teker bütün kombinasyonları denemeye gerek duymayan bir yöntem önermemiz gerekiyor. Eğer en fazla $L$ uzunluğunda olan $N$ adet parçayı teker teker karşılaştırmaya gerek duymadan hizalandırmanın mümkün olduğunu kanıtlayabilirsek, SW metodunun hesapsal karmaşıklığını da böylece düşürebilmiş oluruz.

Bunun yanında, PacBio tarafından üretilen parçaların hata oranının, Illumina tarafından üretilen parçalara kıyasla çok daha yüksek olduğunu belirtmiştik. PacBio için hata oranı \%14 \cite{pacbioerr} ve Illumina parçaları için ise hata oranı \%0.1 - \%1 civarındadır \cite{Lou2013}. Ancak belirttiğimiz gibi Illumina tarafından üretilen parçalar 10 ile 100 katı kadar oranda PacBio parçalarına kıyasla daha küçük olduğu için Illumina ile yineleyen parçalar elde etme olasılığı artmaktadır. Bundan dolayı, güncel gelecek kuşak dizileme teknolojileri için hem en az 10000 baz çifti uzunluğunda hem de çok düşük hata payı oranında parçalar üretmek mümkün değildir. Eğer böyle bir teknolojiye sahip olunsaydı çok daha az yineleyen parçalara sahip olunacağı için, bu durum tüm bir genomu tekrar birleştirme noktasında daha düşük bir hesapsal karmaşıklığa gerek duyardı. 

\section{İlgili Çalışmalar}

Lander-Waterman modeli genom birleştirme yöntemleri içinde en sık kullanılanlardan biridir \cite{Lander1988}. Bu model minimum coverage hakkında tahminleri toplamak için kullanışlı \cite{Lee2014}. Ancak, Lander-Waterman modeli genomda tekrarlayan dizilerin çok sıklıkla var olmadığını varsaydığı için, model tarafından elde edilen tahminler yeteri kadar güçlü veriler olmuyor. Bunun sebebi ise günümüz gelecek kuşak dizileme teknolojilerinin tekrarlayan parça boyutları özellikle 1000 baz çiftinden kısa olmaya başladığı zaman yüksek miktarda tekrarlayan parçalar üretme olasılığıdır.

Lee, Hayan ve ark \cite{Lee2014} tarafından geliştirilen hata düzeltme modelinde izlenen yöntem ise kısa parçaların uzun parçalar üzerinde hizalandırılması üzerine kuruludur. Yukarıda da belirttiğimiz gibi, uzun parçaların tekrarlayan kopyalarının oluşma olasılığı daha düşük olduğundan ve toplam parça sayısı daha az olacağından dolayı hesapsal karmaşıklığı iyileştirmek adına daha uzun parçaların kullanımı tercih edilir. Lee, Hayan ve ark önerdikleri modelde kısa parçaların kullanımını hizalama aşamasında destekleyici bir yapı şeklinde kullanıyorlar. Uzun parçalar üzerinde birleştirilen kısa parçaların, PacBio tarafından üretilen uzun parçaları kullanarak yapılacak genom birleştirme sürecinde bir yarar sağlayacağını savunuyorlar. \cite{Lee2014}. Bu kısa parçaların birleştirme işleminden sonra PacBio tarafından üretilen uzun parçalar üzerinde, kısa parçalar referans alınarak bir hata düzeltme işlemi yaptıklarını belirtiyorlar. Bu hata düzeltme işleminden sonra her ne kadar PacBio parçaları daha az hata barındırarak kullanılabilecek olsa da, Lee, Hayan ve ark SW metodunun hesapsal karmaşıklığındaki üst sınırda herhangi bir iyileştirme sunmuyorlar.

Berlin, Konstantin ve ark ise genom birleştirme sürecindeki hesapsal karmaşıklığı düşürmek adına MinHash birleştirme modelini öneriyorlar \cite{Berlin2015}. Önerdikleri modelde bütün parçaların birbirleriyle karşılaştırılmasını önleyecek bir çözüm sunduklarını belirtiyorlar. Bütün parçaları veya parçaların daha ufak parçalarını indeksleme yöntemiyle sayılardan oluşan parmak izlerini yarattıktan sonra parçaların kendileri yerine bu sayıları karşılaştırma esnasında kullanıyorlar. Böylece karşılaştırma esnasında daha az işlem gücü harcayacak bir yöntem ve uyguladıkları filtrelerle genom birleştirme işlemini $N$ sayıda ve $L$ uzunluğunda parçalar için O($N$$L$) karmaşıklıkta tamamlayabiliyorlar \cite{Berlin2015}. Ancak bütün parçaların $L$ uzunluğunda olacağı varsayımı her zaman uygulanabilir bir varsayım olmayabiliyor. Eğer sadece uzun parçaların genom birleştirme işleminde kullanıldığını belirtmiş olursak, bu varsayım muhtemelen uygulanabilir bir varsayım olabilir. Ancak daha önceden de belirttiğimiz gibi, kısa parçaların uzun parçalar üzerinde hata düzeltme amacında kullanılması genom birleştirme işleminden önce yapılabilecek işlemlerden biridir. Bundan dolayı hem hata oranını düşürmek hem de hesapsal karmaşıklığı düşürmek, bütün parçaların $L$ uzunluğunda olması gerekliliğinden mümkün olamamaktadır.

\section{Amaç}

PacBio tarafından üretilen uzun parçaların ve Illumina tarafından üretilen kısa parçaların kullanımında ne gibi avantajların ve dezavantajların olabileceğini sıklıkla dile getirdik. Açıkça görülebiliyor ki eğer PacBio uzun parçalar üretirken aynı zamanda Illumina'nın sunduğu hata oranında ve maliyette bu parçaları üretebiliyor olsaydı, PacBio, Illumina karşısında daha tercih edilebilir durumda olurdu. Bundan dolayı, Illumina hala market lideri konumunu korumaktadır. Ancak, her iki teknolojinin de bize sunduğu avantajlardan bir hibrit model içinde faydalanmalıyız. Bundan dolayı, ilk amacımız Illumina tarafından üretilen kısa parçaları kullanarak PacBio'nun uzun parçalarının hata oranını düşürmek. Bu şekilde uzun parçaları daha az hata oranıyla kullanabiliyor olacağız.

SW metodunun optimal düzeyde hizalandırma yapabilmesi için gereken hesapsal karmaşıklığı belirtmiştik. Şu anlaşılıyor olmalıdır ki; SW metodunun her ne kadar optimal düzeyde hizalandırma yapmayı başarabilse de elimizde çok fazla sayıda (örneğin milyarlarca) parça olduğu zaman hesapsal karmaşıklığı bu milyarlarca adet parça üzerinde ölçeklenebilir bir boyutta olamıyor. Bundan dolayı büyük genomları tekrar birleştirme aşamasında hesapsal karmaşıklık düzeyi ve ne kadar hassas düzeyde birleştirme yapıldığı arasında bir dengeleme yapmak gerekiyor. Şu anki aşamada en düşük hesapsal karmaşıklık ile en hassas şekilde birleştirme işlemi yapmak belirttiğimiz gibi ölçeklenebilir bir boyutta olmamaktadır. Bundan dolayı ikinci amacımız ise herbir parçanın diğer bütün parçalarla kıyaslanmasını önleyecek bir yöntem geliştirmek. Bu yöntemde hesapsal karmaşıklığı düşürebilmek adına hassaslık seviyesinde yaşayabileceğimiz bazı kayıpları tolera edebiliriz.

\section{Yöntemler}
\subsection{Hata oranının düşürülmesi}
Amacımız, PacBio tarafından üretilen parçaların hata oranını düşürmek olduğundan dolayı, kısa parçaları bu amaç için kullanıyor olacağız. $N_i$ adet ve $L_i$ uzunluğunda, Illumina/Solexa gelecek kuşak dizileme makinası tarafından üretilen parçalarımız var. Aynı zamanda, $N_p$ adet ve $L_p$ uzunluğunda PacBio'nun SMRT makinası tarafından üretilen parçalarımız var. Aynı genom için, Illumina'yı $n_i$ kez çalıştırıyoruz. Eğer her kısa parçanın, $S_{i,i}$, uzun parça(lar), $S_{p,i}$, üzerinde belli pozisyon(lar)a hizalandığını düşünürsek, $j$'inci uzun parça ve $k$ pozisyonu için hizalanan kısa parçaların kümesi aşağıdaki gibi tanımlanabilir:

\begin{equation} \label{eq1}
\forall j \in \{1,2, \dots, N_p\} \hspace{0.3cm} ve \hspace{0.3cm}  0 \leq n \leq N_i \hspace{1cm}  f(S_{p,j}, k) = < S_{i,k_1},  S_i,k_2 , \dots, S_{i,k_n} >
\end{equation}

Böylece $S_{p,j}$ parçasının $k$ pozisyonundaki karakterini, (1)'de belirtilen kümenin içinde yer alan kısa parçaların o karakter üzerinde hizalanmış karakteriyle karşılaştırabiliriz. Eğer kısa parçaların çoğunluğu $k$ pozisyonunda farklı bir karakter olması gerektiğini gösteriyorsa, bu karakteri belirtildiği şekilde düzeltiriz. Illumina tarafından üretilen parçalardaki hata oranının PacBio tarafından üretilen parçalara kıyasla çok daha düşük olduğunu bildiğimizden dolayı, bu karakter düzeltmeleri çoğunlukla PacBio parçalarının hata oranını düşürecek yönde olacaktır.

\subsection{Hesapsal karmaşıklığu azaltmak}
Hata oranını düşürmeden önce çözülmesi gereken problem ise hesapsal karmaşıklığın düşürülmesidir. (1)'de tanımlanan kümeyi oluşturabilmek için etkili bir yöntem önerebilmemiz gerekmektedir. Yöntemimiz, Berlin ve ark tarafından bahsedilen indeksleme yönteminden faydalanmaktadır \cite{Berlin2015}. Toplamda yapılan karşılaştırma sayısını azaltmak için iki adet filtre uyguladıklarını belirtiyorlar. $L$ uzunluğunda parçalar üzerinde bu filtreleri uyguladıkları için kısa parça ve uzun parçayı bir arada kullanmak için bu filtrelerin uygulanabilir olmadığını belirtmiştik. Bundan dolayı öncelikle uzun parçaları, kısa parçaların uzunluğu kadar bölümlere bölüyoruz. Dolayısıyla toplam parça sayısını en az $L_p / L_i$ katı kadar arttırmış oluyoruz. Eğer $S_{p,k}$ PacBio parçasını $l$ uzunlukta artışlarla bölümlere ayıracak olursan, toplam elde edilen bölüm sayısını şu şekilde tanımlayabiliriz:
\begin{gather*}
P_k = ((|S_{p,k}| - |S_{i,k}| )/l) + 1
\end{gather*}
Uzun parçaların kendi aralarında ve kısa parçaların da kendi aralarında ortalama bir uzunluğu olduğunu göz önünde bulundurursak, ortalama bir $P$ değeri belirtmemiz de mümkün olabilir bu sayede. Böylece SW metodu için hesapsal karmaşıklığımız O($P$$N_i$|$S_i|^2$) şeklinde tanımlanabilir. Fingerprint şeklinde adlandırılan ve verilen bir girdiye özel olarak sayısal bir değer veren fonksiyon ile beraber MinHash indeksleme fonksiyonlarının kullanımını içeren iki adet filtreleme uyguluyoruz. Fingerprint fonksiyonunu ve Minhash fonksiyonunu kullanmadan önce bütün kısa ve uzun parçaların k-merlerini yaratıyoruz. Sırasıyla kısa ve uzun parçaların k-mer kümelerini ise $K_{i,j}$ ve $K_{p,j}$ şeklinde tanımlıyoruz. Bu kümelerin boyutu ise
\begin{gather*}
| K_{(p | i), j} | = |S_{(p | i),j}| - k + 1
\end{gather*}
şeklinde tanımlanabilir. K-merleri fingerprint fonksiyonlarına sokmak için öncelikle pozitif bir $H$ değeri belirliyoruz. $H$ değeri, herbir k-meri kaç defa fingerprint fonksiyonuna soktuğumuzu belirtiyor. Fingerprint fonksiyonunu bütün k-merler için bir defa çalıştırdıktan sonra, bir sonraki çalıştırma esnasında aynı k-mer için farklı değer üretildiğinden emin olabilmek için önceki çalıştırmalarda verilmeyen bir besleme değeri veriyoruz. Bütün parçaların k-merlerini $H$ defa fingerprint fonksiyonuna soktuğumuzu düşünürsek, bu işlemi yapmanın hesapsal karmaşıklığı O(|$K$|$NH$) şeklinde belirtilebilir. Bu aşamadan sonra fingerprint fonksiyonlarından üretilen sayısal değerlere bakaram min-meri buluyoruz. Min-mer olarak belirtilen değer, $h$nci fingerprint fonksiyonunun çalıştırılması esnasında en düşük positif fingerprint değerinin atandığı k-mere denk gelmektedir. Bütün min-mer değerlerini $sketch$ adı verilen bir dizide tutuyoruz. Açıkça görülebileceği gibi, her bir parça için $H$ adet fingerprint değeri üretildiği için, bu dizinin büyüklüğü de $H$ kadardır. Min-mer değerleri $sketch$ içinde toplandıktan sonra, iki parça arasındaki benzerliği Jaccard benzerliği \cite{Jaccard1901} hesaplamasıyla buluyoruz. Bizim durumumuzda iki parça sırasıyla kısa ve uzun parçalar olmak üzere $S_{i,m}$, $S_{p,n}$ şeklinde isimlendirilebilir. Berlin ve ark $h$nci fingerprint fonksiyonu kullanılarak oluşan kümelerden iki parçanın benzerliğini
\begin{equation}
J(S_{i,m}, S_{p,n}) = \dfrac{|\Gamma_{h}(S_{i,m}) \cap \Gamma_{h}(S_{p,n})|}{|\Gamma_{h}(S_{i,m}) \cup \Gamma_{h}(S_{p,n})|}
\end{equation}
şeklinde tanımlamaktadır. Eğer, (2)'de $w$ kadar bir kesişim olduğunu varsayarsak, (2) numaralı eşitliği şu şekilde de yazmamız mümkün olabilir:
\begin{equation}
J(S_{i,m}, S_{p,n}) = \dfrac{w}{2K - w}
\end{equation}
Bu benzerliği yaratılan $sketch$ dizisi için de kullanabiliriz. Eğer $w$ iki parça için de ortaya çıkan ortak min-mer sayısını gösteriyorsa, iki parçanın Jaccard benzerliği şu şekilde yazılabilir:
\begin{equation}
J(S_{i,m}, S_{p,n}) = \dfrac{w}{2H - w}
\end{equation}
Bizim yöntemimizde, bu benzerliği birinci filtre uygulanırken bir eşik değeri oluşturmak için kullanıyoruz. $sketch$ dizisini oluşturma yöntemimiz sayesinde Jaccard benzerliğini O($HN_i$|$K_i$|P) zaman içinde hesaplayabilmekteyiz. Sözlük veri yapısını (anahtar - değer çifleri) kullanarak $sketch$ dizilerini yaratmaktayız. Herbir $h$nci fingerprint fonksiyonu için, bir sözlük veri yapısı oluştruyoruz. Anahtar olarak herhangi bir parçanın k-meri için ortaya çıkmış min-mer değerini kullanırken bu anahtarın değeri olarak da $h$nci fingerprint fonksiyonunda bu min-mere sahip olan parçaların kümesini tutmaktayız. Kısa ve uzun parçaların $sketch$ dizilerini ayrı ayrı yaratıyoruz. Daha sonradan, kısa parçaların $h$nci sözlükteki herbir min-mer değeri için, aynı min-mer değerinin uzun parçalar için yaratılan $h$nci sözlükte anahtar olarak yer alıp almadığını kontrol ediyoruz. Bir uzun parçanın en fazla ortak sayıda min-mere sahip bölümünü Jaccard benzerliği hesaplanırken ele almaktayız. Örneğin $k$nci uzun parçanın $m$nci bölümünün içinde yer alan min-merler, aynı parçanın diğer herhangi bir bölümünde yer alan min-merlerine kıyasla $l$nci kısa parçada daha fazla ortak min-mere sahipse $S_{i,l}$ $S_{p,k}$ olarak bu bölümde bulunan min-merler ve k-merler referans alınır. Bütün anahtarlar belirtilen yöntemle kontrol edildikten sonra Jaccard benzerliğini hesaplama aşamasına geçiyoruz. Kısa - uzun parça eşlerinin Jaccard benzerliği eğer belirtilen $T$ eşik değerinden fazla ise ikinci filtrede değerlendirilmek üzere işaretleniyorlar. Böylece birinci filtrede bazı uzun-kısa parçaları SW metodunda her parçayı diğer bütün parçalarla kıyaslamamak için elemiş oluyoruz.
İkinci filtre temel olarak ilk filtreye çok benzemektedir. Tek fark olarak artık karşılaştırma esnasında ortak min-merleri değil direkt olarak ortak k-merleri karşılaştırıyoruz. Yine benzer olarak, eğer bir kısa-uzun parça eşi belirli sayıda ortak k-mere sahipse Jaccard benzerliği eşiğini geçerek ikinci filtreyi geçmiş olarak işaretliyoruz bu eşleri. İkinci filtreyi uygulamızdaki esas amaç daha hassas elemeler yapabilecek olmamızdır. Direkt olarak k-merlerin karşılaştırılması, fingerprint fonksiyonlarından çıkan min-mer değerlerinin bulunup onların karşılaştırılmasına kıyasla daha kesin bir benzerlik oranı yaratacaktır. Ancak bütün k-merleri karşılaştırmak ilk aşamada min-merleri karşılaştırmaktan daha fazla bir hesapsal karmaşıklık yaratacağından dolayı, ilk filtreyle hızlı bir eleme yaptıktan sonra kalan eşlerle karşılaştırma yapmaktayız. Açıkça görüleceği gibi bu iki filtre de, hizalandırma aşamasında bizim bütün parçaların diğer bütün parçalarla karşılaştırılmasının önüne geçecektir ve O($P$$N_i$|$S_i|^2$) hesapsal karmaşıklıktan kurtaracaktır. Eğer ikinci filterede toplam k-mer sayısının sırasıyla kısa ve uzun parçalar için $K_{i,tot}$ and $K_{p,tot}$ olduğunu varsayarsak, hesapsal karmaşıklık O( $K_{i,tot}$ $K_{p,tot}$) olacaktır. Burada vurgulamamız gereken nokta ise toplam k-mer sayıları, bütün parçaların diğer bütün parçalarla kıyaslanma sayısından çok daha düşüktür. Bundan dolayı, bu iki filtre böylece SW metoduyla gelen hesapsal karmaşıklığı azaltarak daha az kıyaslamanın yapılmasını münmkün kılmaktadır.
